# Sequence-to-Sequence-Learning
Implementing sequence to sequence learning using an LSTM based encoder-decoder style architecture using Keras with a Theano backend. This is an ongoing project and to achieve the target, smaller milestones have been set with the overarching aim of not only implementing sequence to sequence learning but also to enable broader understanding of implementing architectures in Keras.

- [x] **Milestone 1:** 
Given a sample of text, an LSTM layer learns and starts generating the text (character by character) in the style of the author. The output, given a random seed is as shown:

`oral to be in the his art in the sublict and in the subler and and to the ore and the his to the his been the for the his pore of the some the some and and the the the has the respending the some the here to the here the inderstand and to the some the some the some and the presance and the some the his an and the some his to the himself the are are the himself to the some his to the here to the lo`

- [ ] **Milestone 2:**
Modify this architecture to incorporate a word embedding layer (ex: word2vec) and perform the same task.

- [ ] **Milestone 3:**
Create an encoder LSTM layer and decoder layer to learn sequences within the same language

- [ ] **Milestone 4:**
Using a different bag of words for the decoder layer and learn translations from English to French.
